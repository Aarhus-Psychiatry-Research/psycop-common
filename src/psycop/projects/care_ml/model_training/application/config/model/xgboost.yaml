# @package _global_
model:
  name: xgboost
  require_imputation: false
  args:
    n_estimators: 433
    tree_method: gpu_hist # set to gpu_hist to enable GPU training (default auto)
    booster: gbtree
    random_state: 42
    alpha: 0.05442377426915552
    gamma: 0.00022890588912611955
    lambda: 0.3333089630085611
    learning_rate: 0.024525414190942597
    max_depth: 5
    grow_policy: "lossguide"

# Parameters that will only take effect if running with --multirun
hydra:
  sweeper:
    params:
      ++model.args.n_estimators: int(tag(log, interval(100, 1200)))
      ++model.args.alpha: tag(log, interval(1e-8, 0.1))
      ++model.args.lambda: tag(log, interval(1e-8, 1.0))
      ++model.args.max_depth: int(interval(1, 10))
      ++model.args.learning_rate: tag(log, interval(1e-8, 1)) # Multiplier during boosting, [0,1]. Lower numbers mean more conservative boosting. Default is 0.3
      ++model.args.gamma: tag(log, interval(1e-8, 0.001)) # Threshold for loss reduction per node split. If lower than threshold, stops adding nodes to branch.
      ++model.args.grow_policy: choice("depthwise", "lossguide")
      ++model.args.scale_pos_weight: choice(1, 0.004766812677145065, 0.06904210800044466) # 1, count(negative examples)/count(positive examples), sqrt(count(negative examples)/count(positive examples))
