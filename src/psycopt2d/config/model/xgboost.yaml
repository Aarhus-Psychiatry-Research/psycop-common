# @package _global_
model:
  model_name: xgboost # (str): Model, can currently take xgboost
  require_imputation: true # (bool): Whether the model requires imputation. (shouldn't this be false?)
  args:
    n_estimators: 100
    tree_method: auto # set to gpu_hist to enable GPU training (default auto)

# Parameters that will only take effect if running with --multirun
hydra:
  sweeper:
    params:
      ++model.args.n_estimators: choice(100, 300, 500, 800, 1200)
      ++model.args.lambda: tag(log, interval(1e-8, 1.0))
      ++model.args.alpha: tag(log, interval(1e-8, 1.0))
      ++model.args.booster: choice("gbtree")
      ++model.args.max_depth: int(interval(3, 8))
      ++model.args.learning_rate: tag(log, interval(1e-8), 0.3)) # Multiplier during boosting, [0,1]. Lower numbers mean more conservative boosting. Default is 0.3
      ++model.args.gamma: tag(log, interval(1e-4, 0.1)) # Threshold for loss reduction per node split. If lower than threshold, stops adding nodes to branch.
      ++model.args.grow_policy: choice("depthwise", "lossguide")