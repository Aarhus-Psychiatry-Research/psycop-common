# @package _global_
model:
  model_name: xgboost # (str): Model, can currently take xgboost
  require_imputation: true # (bool): Whether the model requires imputation. (shouldn't this be false?)
  args:
    n_estimators: 100
    tree_method: auto # set to gpu_hist to enable GPU training (default auto)

# Parameters that will only take effect if running with --multirun
hydra:
  sweeper:
    params:
      ++model.args.n_estimators: choice(100, 300, 500, 800, 1200)
      ++model.args.lambda: tag(log, interval(1e-4, 1.0))
      ++model.args.alpha: tag(log, interval(1e-4, 1.0))
      ++model.args.booster: choice("gbtree", "dart")
      ++model.args.max_depth: int(interval(1, 9))
      ++model.args.learning_rate: tag(log, interval(0.1, 0.5)) # Multiplier during boosting, [0,1]. Lower numbers mean more conservative boosting. Default is 0.3
      ++model.args.gamma: tag(log, interval(1e-4, 0.1)) # Minimum loss reduction required to make a further partition on a leaf node of the tree. If lower, stops adding nodes.
      ++model.args.grow_policy: choice("depthwise", "lossguide")