import datetime as dt
import platform
from pathlib import Path

import pytest
from torch import nn
from torch.utils.data import DataLoader

from psycop.common.data_structures.patient import PatientSlice
from psycop.common.data_structures.prediction_time import PredictionTime
from psycop.common.sequence_models.aggregators import Aggregator, AveragePooler, CLSAggregator
from psycop.common.sequence_models.optimizers import LRSchedulerFn, OptimizerFn
from psycop.common.sequence_models.tasks.patientslice_classifier import PatientSliceClassifier

from ..dataset import PredictionTimeDataset
from ..embedders.BEHRT_embedders import BEHRTEmbedder
from ..tasks.pretrainer_behrt import PretrainerBEHRT


def arm_within_docker() -> bool:
    """Check if we are running on ARM within docker. ARM on Mac is "arm64", and non-arm on Linux is "x86_64"""
    return "aarch64" in platform.machine()


skip_if_arm_within_docker = pytest.mark.skipif(
    arm_within_docker(),
    reason="Skipping test on ARM within docker. Some tests fail, unknown reason, see https://github.com/Aarhus-Psychiatry-Research/psycop-common/issues/348",
)


@pytest.fixture
def patient_dataset_with_labels(patient_slices: list[PatientSlice]) -> PredictionTimeDataset:
    prediction_times = []
    for i, patient_slice in enumerate(patient_slices):
        prediction_times.append(
            PredictionTime(
                patient_slice=patient_slice,
                prediction_timestamp=dt.datetime(year=2000 + i, month=1, day=1),
                outcome=i % 2 == 0,
            )
        )

    return PredictionTimeDataset(prediction_times=prediction_times)


parametrise_aggregator = pytest.mark.parametrize("aggregator", [CLSAggregator(), AveragePooler()])


def _run_backward_pass(dataloader: DataLoader[PredictionTime], clf: PatientSliceClassifier) -> None:
    for input_ids, labels in dataloader:
        logits = clf.forward(input_ids)
        loss = clf._calculate_loss(labels=labels, logits=logits)  # type: ignore[privateImportUsage]
        loss.backward()  # ensure that the backward pass works


@skip_if_arm_within_docker
@parametrise_aggregator
def test_encoder_for_clf(
    patient_dataset_with_labels: PredictionTimeDataset,
    embedder: BEHRTEmbedder,
    encoder: nn.Module,
    aggregator: Aggregator,
    optimizer: OptimizerFn,
    lr_scheduler_fn: LRSchedulerFn,
):
    clf = PatientSliceClassifier(
        embedder=embedder,
        encoder=encoder,
        aggregator=aggregator,
        num_classes=2,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler_fn,
    )

    dataloader = DataLoader(
        patient_dataset_with_labels, batch_size=32, shuffle=True, collate_fn=clf.collate_fn
    )
    _run_backward_pass(dataloader=dataloader, clf=clf)


@parametrise_aggregator
def test_encoder_for_clf_for_multiclass(
    patient_dataset_with_labels: PredictionTimeDataset,
    embedder: BEHRTEmbedder,
    encoder: nn.Module,
    aggregator: Aggregator,
    optimizer: OptimizerFn,
    lr_scheduler_fn: LRSchedulerFn,
):
    clf = PatientSliceClassifier(
        embedder=embedder,
        encoder=encoder,
        aggregator=aggregator,
        num_classes=4,  # more than 2 classes
        optimizer=optimizer,
        lr_scheduler=lr_scheduler_fn,
    )

    dataloader = DataLoader(
        patient_dataset_with_labels, batch_size=32, shuffle=True, collate_fn=clf.collate_fn
    )

    _run_backward_pass(dataloader=dataloader, clf=clf)


TEST_CHECKPOINT_DIR = Path(__file__).parent / "test_checkpoints" / "checkpoints"


@skip_if_arm_within_docker
@parametrise_aggregator
def test_pretrain_from_checkpoint(
    patient_dataset_with_labels: PredictionTimeDataset,
    aggregator: Aggregator,
    optimizer: OptimizerFn,
    lr_scheduler_fn: LRSchedulerFn,
):
    """
    Check whether we can continue pre-training from an existing checkpoint.

    If this test fails it mean that we have lost backwards compatibility with previous checkpoints and you will
    need to recreate a new checkpoint. To regenerate the checkpoint, you can use the checkpoint generated by
    test_behrt: test_module_with_trainer
    """
    checkpoint_path = next(TEST_CHECKPOINT_DIR.glob("*.ckpt"))

    loaded_model = PretrainerBEHRT.load_from_checkpoint(checkpoint_path)

    clf = PatientSliceClassifier(
        embedder=loaded_model.embedder,
        encoder=loaded_model.encoder,
        aggregator=aggregator,
        num_classes=2,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler_fn,
    )

    dataloader = DataLoader(
        patient_dataset_with_labels, batch_size=32, shuffle=True, collate_fn=clf.collate_fn
    )

    _run_backward_pass(dataloader=dataloader, clf=clf)
