[training]
batch_size=64
num_workers_for_dataloader=2

[training.trainer]
accelerator = "auto"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision = "32-true"
max_epochs = 2
min_epochs = null
max_steps = 10
min_steps = null
limit_train_batches = null
limit_val_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 2
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "logs/"


[training.trainer.callbacks]
@callbacks = "callback_list"

[training.trainer.callbacks.*.model_checkpoint]
@callbacks = "model_checkpoint"
monitor = "val_loss"
save_top_k = 5
every_n_epochs = 1
mode = "min"
save_dir = ${logger.*.wandb.save_dir}

[training.trainer.callbacks.*.learning_rate_monitor]
@callbacks = "learning_rate_monitor"
logging_interval = "epoch"


[logger]
@utilities = "list_creator"

[logger.*.wandb]
@loggers = "wandb"
save_dir = "logs/"
experiment_name = "test_experiment"
run_name = "test_run"
offline = true

[model_and_dataset]
[model_and_dataset.model]
@tasks = "behrt"

[model_and_dataset.model.optimizer]
@optimizers = "adam"
lr = 0.03

[model_and_dataset.model.lr_scheduler]
@lr_schedulers = "linear_schedule_with_warmup"
num_warmup_steps = 0
num_training_steps = 10


[model_and_dataset.model.embedder]
@embedders = "behrt_embedder"
d_model = 32
dropout_prob = 0.1
max_sequence_length = 128

[model_and_dataset.model.encoder]
@layers = "transformer_encoder"
num_layers = 2

[model_and_dataset.model.encoder.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model_and_dataset.model.embedder.d_model}
nhead = 8
dim_feedforward = 128
layer_norm_eps = 1e-12
norm_first = true


[model_and_dataset.training_dataset]
@datasets = "test_dataset"

[model_and_dataset.validation_dataset]
@datasets = "test_dataset"
