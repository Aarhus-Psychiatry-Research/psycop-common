[logger]
@utilities = "list_creator"

[logger.*.mlflow]
@loggers = "mlflow"
experiment_name = "t2d_behrt_finetune_with_pretrain"
run_name = "finetune-with-regional-splits"
save_dir = "E:/shared_resources/sequence_models/BEHRT/t2d_behrt_finetune_with_pretrain/finetune-regional-splits/"

[training]
batch_size=256
num_workers_for_dataloader=8

[training.trainer]
accelerator = "gpu"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision = "bf16-mixed"
max_epochs = null
min_epochs = null
max_steps = 100000
min_steps = null
limit_train_batches = null
limit_val_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = 0
log_every_n_steps = 20
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "logs/"
logger=${logger.*.mlflow}

[training.trainer.callbacks]
@callbacks = "callback_list"

[training.trainer.callbacks.*.model_checkpoint]
@callbacks = "model_checkpoint"
monitor = "val_loss"
save_top_k = 5
every_n_epochs = 1
mode = "min"
save_dir = ${logger.*.mlflow.save_dir}

[training.trainer.callbacks.*.learning_rate_monitor]
@callbacks = "learning_rate_monitor"
logging_interval = "epoch"

#########
# Model #
#########
[pretrained_model]
checkpoint_path = "E:/shared_resources/sequence_models/BEHRT/pretrain_behrt/pretrain-regional-splits/checkpoints/epoch=117-step=23128.ckpt"

[pretrained_model.embedder]
@tasks = "embedder_from_checkpoint"
checkpoint_path = ${pretrained_model.checkpoint_path}

[pretrained_model.encoder]
@tasks = "encoder_from_checkpoint"
checkpoint_path = ${pretrained_model.checkpoint_path}


[model_and_dataset]
[model_and_dataset.model]
@tasks = "patient_slice_classifier"
embedder = ${pretrained_model.embedder}
encoder = ${pretrained_model.encoder}
aggregator = {"@layers":"cls_aggregator"}

[model_and_dataset.model.optimizer]
@optimizers = "adam"
lr = 0.03

[model_and_dataset.model.lr_scheduler]
@lr_schedulers = "linear_schedule_with_warmup"
num_warmup_steps = 1000
num_training_steps = ${training.trainer.max_steps}


###########
# Dataset #
###########
[cohort_definer]
[cohort_definer.cohort]
@cohorts = "t2d"

[psycop_event_loaders]
@utilities = "list_creator"

[psycop_event_loaders.*.diagnoses]
@event_loaders = "diagnoses"

# Training
[model_and_dataset.training_dataset]
@datasets = "prediction_time_collater"
cohort_definer = ${cohort_definer.cohort}
lookbehind_days=730
lookahead_days=730

[model_and_dataset.training_dataset.patient_loader]
@datasets = "patient_loader"
event_loaders = ${psycop_event_loaders}
min_n_events=5

[model_and_dataset.training_dataset.patient_loader.split_filter]
@preprocessing = "regional_data_filter"
splits_to_keep = ["train"]

# Validation
[model_and_dataset.validation_dataset]
@datasets = "prediction_time_collater"
cohort_definer = ${cohort_definer.cohort}
lookbehind_days=730
lookahead_days=730

[model_and_dataset.validation_dataset.patient_loader]
@datasets = "patient_loader"
event_loaders = ${psycop_event_loaders}
min_n_events=5

[model_and_dataset.validation_dataset.patient_loader.split_filter]
@preprocessing = "regional_data_filter"
splits_to_keep = ["val"]