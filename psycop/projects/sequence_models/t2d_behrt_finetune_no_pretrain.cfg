[training]
batch_size=256
num_workers_for_dataloader=8

[training.trainer]
accelerator = "gpu"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision = "bf16-mixed"
max_epochs = null
min_epochs = null
max_steps = 100000
min_steps = null
limit_train_batches = null
limit_val_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 10
num_sanity_val_steps = 0
log_every_n_steps = 2
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = ${training.trainer.logger.save_dir}

[training.trainer.logger]
@loggers = "mlflow"
experiment_name = "pretrain_behrt"
run_name = "pretrain-test"
save_dir = "E:/shared_resources/sequence_models/T2D_no_pretrain/"

[training.trainer.callbacks]
@callbacks = "callback_list"

[training.trainer.callbacks.*.model_checkpoint]
@callbacks = "model_checkpoint"
monitor = "val_loss"
save_top_k = 5
every_n_epochs = 1
mode = "min"
save_dir = ${training.trainer.logger.save_dir}

[training.trainer.callbacks.*.learning_rate_monitor]
@callbacks = "learning_rate_monitor"
logging_interval = "epoch"


[cohort_definer]
[cohort_definer.cohort]
@cohorts = "t2d"

[event_loader]
@utilities = "list_creator"

[event_loader.*.diagnoses]
@event_loaders = "diagnoses"

[model_and_dataset]
[model_and_dataset.training_dataset]
@datasets = "prediction_time_collater"
cohort_definer = ${cohort_definer.cohort}
event_loaders = ${event_loader}
split_name = "train"
lookbehind_days = 730
lookahead_days = 730

[model_and_dataset.validation_dataset]
@datasets = "prediction_time_collater"
cohort_definer = ${cohort_definer.cohort}
event_loaders = ${event_loader}
split_name = "val"
lookbehind_days = 730
lookahead_days = 730

[model_and_dataset.model]
@tasks = "patient_slice_classifier"

[model_and_dataset.model.aggregator]
@layers = "cls_aggregator"

[model_and_dataset.model.optimizer]
@optimizers = "adam"
lr = 0.03

[model_and_dataset.model.lr_scheduler]
@lr_schedulers = "linear_schedule_with_warmup"
num_warmup_steps = 1000
num_training_steps = ${training.trainer.max_steps}

[model_and_dataset.model.embedder]
@embedders = "behrt_embedder"
d_model = 288
dropout_prob = 0.1
max_sequence_length = 256

[model_and_dataset.model.encoder]
@layers = "transformer_encoder"
num_layers = 6

[model_and_dataset.model.encoder.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model_and_dataset.model.embedder.d_model}
nhead = 12
dim_feedforward = 512
layer_norm_eps = 1e-12
norm_first = false