# @package _global_
model:
  name: xgboost
  require_imputation: false
  args:
    n_estimators: 784
    tree_method: gpu_hist # set to gpu_hist to enable GPU training (default auto)
    booster: gbtree
    random_state: 42
    alpha: 3.275298486659782e-06
    gamma: 3.887255647581438e-06
    lambda: 2.8183352713018024e-05
    learning_rate: 0.017361468414342048
    max_depth: 3
    grow_policy: "depthwise"
    scale_pos_weight: 15.893695882731569

# Parameters that will only take effect if running with --multirun
hydra:
  sweeper:
    params:
      ++model.args.n_estimators: int(tag(log, interval(100, 1200)))
      ++model.args.alpha: tag(log, interval(1e-8, 0.1))
      ++model.args.lambda: tag(log, interval(1e-8, 1.0))
      ++model.args.max_depth: int(interval(1, 10))
      ++model.args.learning_rate: tag(log, interval(1e-8, 1)) # Multiplier during boosting, [0,1]. Lower numbers mean more conservative boosting. Default is 0.3
      ++model.args.gamma: tag(log, interval(1e-8, 0.001)) # Threshold for loss reduction per node split. If lower than threshold, stops adding nodes to branch.
      ++model.args.grow_policy: choice("depthwise", "lossguide")
      ++model.args.scale_pos_weight: choice(1, 252.60956881275843, 15.893695882731569) # 1, count(negative examples)/count(positive examples), sqrt(count(negative examples)/count(positive examples))
