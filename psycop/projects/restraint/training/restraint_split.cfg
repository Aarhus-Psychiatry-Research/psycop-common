[project_info]
experiment_path = "E:/shared_resources/coercion/split_v2/"

[logger]
@loggers = "multi_logger"

[logger.*.terminal]
@loggers = "terminal_logger"

[logger.*.mlflow]
@loggers = "mlflow_logger"
experiment_name = "restraint_split_trainer"
postpone_run_creation_to_first_log = True

[logger.*.disk_logger]
@loggers = "disk_logger"
experiment_path = ${project_info.experiment_path}

[trainer]
@trainers = "split_trainer"
uuid_col_name = "prediction_time_uuid"
training_outcome_col_name = "outc_all_restraint_within_2_days_boolean_fallback_0_dichotomous"
validation_outcome_col_name = "outc_mechanical_restraint_within_2_days_boolean_fallback_0_dichotomous"
n_splits = 5
group_col_name = "dw_ek_borger"

[trainer.metric]
@metrics = "binary_auroc"

[trainer.training_data]
@data = "parquet_vertical_concatenator"
paths = ["E:/shared_resources/coercion/flattened_datasets/train.parquet", "E:/shared_resources/coercion/flattened_datasets/val.parquet", "E:/shared_resources/coercion/flattened_datasets/test.parquet"]


[trainer.preprocessing_pipeline.*.regional_data_filter]
@preprocessing = "regional_data_filter"
splits_to_keep = ["train", "val"]


#################
# Preprocessing #
#################
[trainer.preprocessing_pipeline]
@preprocessing = "baseline_preprocessing_pipeline"

[trainer.preprocessing_pipeline.*.bool_to_int]
@preprocessing = "bool_to_int"

# Filter rows


# Filter columns
## Predictors
# [trainer.preprocessing_pipeline.*.layer_selector]
# @preprocessing = "filter_columns_within_subset"
# subset_rule = "pred_.+layer.+"
# keep_matching = ".+_layer_(1|2|3).+"

# # [trainer.preprocessing_pipeline.*.aggregation_selector]
# # @preprocessing = "filter_columns_within_subset"
# # subset_rule = "pred_.+layer.+"
# # keep_matching = ".+_(mean|max|min)_.+"

## Outcomes
[trainer.preprocessing_pipeline.*.outcome_selector]
@preprocessing = "filter_columns_within_subset"
subset_rule = "outc_.+"
keep_matching = ".+mechanical.+"

## Remove timestamp columns
[trainer.preprocessing_pipeline.*.temporal_col_filter]
@preprocessing = "temporal_col_filter"

# Validate column assumptions
[trainer.preprocessing_pipeline.*.columns_exist]
@preprocessing = "column_exists_validator"
column_names = ["prediction_time_uuid", "pred_age_in_years", "pred_sex_female"]

[trainer.preprocessing_pipeline.*.column_prefix_count_expectation]
@preprocessing = "column_prefix_count_expectation"
column_expectations = [["outc_", 1], ["prediction_timestamp", 0]]

#############
# Estimator #
#############
# [trainer.task.task_pipe.sklearn_pipe.*.model]
# @estimator_steps_suggesters = "lightgbm_suggester"

########
# Task #
########
[trainer.task]
@tasks = "binary_classification"

[trainer.task.task_pipe]
@task_pipelines = "binary_classification_pipeline"

[trainer.task.task_pipe.sklearn_pipe]
@task_pipelines = "pipe_constructor"

# [trainer.task.task_pipe.sklearn_pipe.*.model]
# @estimator_steps = "xgboost"

# [trainer.task.task_pipe.sklearn_pipe.*.model]
# @estimator_steps = "xgboost"
# alpha = 0
# reg_lambda = 1
# max_depth = 3
# learning_rate = 0.3
# gamma = 0
# tree_method = "gpu_hist",
# n_estimators = 100