[logger]
@loggers = "multi_logger"

[logger.*.mlflow_logger]
@loggers = "mlflow_logger"
experiment_name = "sczbp/test_tfidf_1000"
postpone_run_creation_to_first_log = "True"

[logger.*.disk_logger]
@loggers = "disk_logger"
run_path = "E:/shared_resources/scz_bp/testing/"

[trainer]
@trainers = "synthetic_crossval_trainer"
outcome_col_name = "outc_first_scz_or_bp_within_1825_days_maximum_fallback_0_dichotomous"
n_splits = 5
uuid_col_name = "prediction_time_uuid"
group_col_name = "dw_ek_borger"
metric = {"@metrics":"binary_auroc"}


[trainer.training_data]
@data = "parquet_vertical_concatenator"
paths = ["E:/shared_resources/scz_bp/flattened_datasets/l1_l4-lookbehind_183_365_730-all_relevant_tfidf_1000_lookbehind_730.parquet"]

[trainer.synthetic_data]
@data = "synthetic_data_vertical_concatenator"
paths = ["E:/shared_resources/scz_bp/synthetic_data/dppm_lr_0.0005_n_iter_4000_n_timesteps_500.0_only_positive_149140.parquet"]
n_samples = 59656

### Trainig preprocessing
[trainer.preprocessing_pipeline]
@preprocessing = "baseline_preprocessing_pipeline"

[trainer.preprocessing_pipeline.*.split_filter]
@preprocessing = "regional_data_filter"
splits_to_keep = ["train","val"]
regional_move_df = null
timestamp_col_name = "timestamp"
id_col_name = "dw_ek_borger"
region_col_name = "region"
timestamp_cutoff_col_name = "first_regional_move_timestamp"

[trainer.preprocessing_pipeline.*.lookahead_distance_filter]
@preprocessing = "window_filter"
n_days = 1825
direction = "ahead"
timestamp_col_name = "timestamp"

[trainer.preprocessing_pipeline.*.age_filter]
@preprocessing = "age_filter"
min_age = 15
max_age = 60
age_col_name = "pred_age_in_years"

[trainer.preprocessing_pipeline.*.regex_meta_blacklist]
@preprocessing = "regex_column_blacklist"
* = ["meta_.*"]

[trainer.preprocessing_pipeline.*.columns_exist]
@preprocessing = "column_exists_validator"
column_names = ["pred_age_in_years","prediction_time_uuid"]

[trainer.preprocessing_pipeline.*.outcome_selector]
@preprocessing = "filter_columns_within_subset"
subset_rule = "outc_.+"
keep_matching = ".+1825.+"

[trainer.preprocessing_pipeline.*.temporal_col_filter]
@preprocessing = "temporal_col_filter"

# [trainer.preprocessing_pipeline.*.layer_selector]
# @preprocessing = "filter_columns_within_subset"
# subset_rule = "pred_.+layer.+"
# keep_matching = ".+_layer_(5).+"

[trainer.preprocessing_pipeline.*.column_prefix_count_expectation]
@preprocessing = "column_prefix_count_expectation"
column_expectations = [["outc_",1]]

[trainer.preprocessing_pipeline.*.bool_to_int]
@preprocessing = "bool_to_int"




### Task and model
[trainer.task]
@tasks = "binary_classification"

[trainer.task.task_pipe]
@task_pipelines = "binary_classification_pipeline"

[trainer.task.task_pipe.sklearn_pipe]
@task_pipelines = "pipe_constructor"

# [trainer.task.task_pipe.sklearn_pipe.*.imputer]
# @estimator_steps = "simple_imputation"
# strategy = "mean"

# [trainer.task.task_pipe.sklearn_pipe.*.model]
# @estimator_steps = "xgboost"
# n_estimators = 496
# alpha = 0.0000993632
# reg_lambda = 0.0081552569
# max_depth = 7
# learning_rate = 0.0114594709
# gamma = 0.0000001592

[trainer.task.task_pipe.sklearn_pipe.*.imputer]
@estimator_steps = "simple_imputation"
strategy = "most_frequent"

[trainer.task.task_pipe.sklearn_pipe.*.model]
@estimator_steps = "xgboost"
n_estimators = 760
alpha = 0.0034144595
reg_lambda = 0.0000004426
max_depth = 3
learning_rate = 0.0282324618
gamma = 0.0000071143